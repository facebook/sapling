/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This software may be used and distributed according to the terms of the
 * GNU General Public License version 2.
 */

#![feature(never_type)]
#![type_length_limit = "2000000"]

use anyhow::Error;
use bytes::Bytes;
use bytes::BytesMut;
use cloned::cloned;
use futures::future::Future;
use futures::future::TryFutureExt;
use futures::stream;
use futures::stream::Stream;
use futures::stream::TryStreamExt;
use std::borrow::Borrow;

use blobstore::Blobstore;
use blobstore::Loadable;
use blobstore::LoadableError;
use context::CoreContext;
use mononoke_types::hash;
use mononoke_types::BlobstoreKey;
use mononoke_types::ContentId;
use mononoke_types::ContentMetadata;
use mononoke_types::FileContents;

mod alias;
mod chunk;
mod copy;
mod errors;
mod expected_size;
mod fetch;
mod fetch_key;
mod finalize;
mod incremental_hash;
mod metadata;
mod multiplexer;
mod prepare;
mod rechunk;
mod streamhash;

pub use copy::copy;
pub use copy::BlobCopier;
pub use fetch::Range;
pub use fetch_key::Alias;
pub use fetch_key::AliasBlob;
pub use fetch_key::FetchKey;
pub use rechunk::force_rechunk;
pub use rechunk::rechunk;

#[cfg(test)]
mod test;

/// File storage.
///
/// This is a specialized wrapper around a blobstore specifically for user data files (rather
/// rather than metadata, trees, etc). Its primary (initial) goals are:
/// - providing a streaming interface for file access
/// - maintain multiple aliases for each file using different key schemes
/// - maintain reverse mapping from primary key to aliases
///
/// Secondary:
/// - Implement chunking at this level
/// - Compression
/// - Range access (initially fetch, and later store)
///
/// Implementation notes:
/// This code takes over the management of file content in a backwards compatible way - it uses
/// the same blobstore key structure and the same encoding schemes for existing files.
/// Extensions (compression, chunking) will change this, but it will still allow backwards
/// compatibility.
#[facet::facet]
#[derive(Debug, Copy, Clone)]
pub struct FilestoreConfig {
    pub chunk_size: Option<u64>,
    pub concurrency: usize,
}

impl FilestoreConfig {
    pub fn no_chunking_filestore() -> Self {
        Self {
            chunk_size: None,
            concurrency: 1,
        }
    }
}

/// Key for storing. We'll compute any missing keys, but we must have the total size.
#[derive(Debug, Clone)]
pub struct StoreRequest {
    pub expected_size: expected_size::ExpectedSize,
    pub canonical: Option<ContentId>,
    pub sha1: Option<hash::Sha1>,
    pub sha256: Option<hash::Sha256>,
    pub git_sha1: Option<hash::RichGitSha1>,
}

impl StoreRequest {
    pub fn new(size: u64) -> Self {
        use expected_size::ExpectedSize;

        Self {
            expected_size: ExpectedSize::new(size),
            canonical: None,
            sha1: None,
            sha256: None,
            git_sha1: None,
        }
    }

    pub fn with_canonical(size: u64, canonical: ContentId) -> Self {
        use expected_size::ExpectedSize;

        Self {
            expected_size: ExpectedSize::new(size),
            canonical: Some(canonical),
            sha1: None,
            sha256: None,
            git_sha1: None,
        }
    }

    pub fn with_sha1(size: u64, sha1: hash::Sha1) -> Self {
        use expected_size::ExpectedSize;

        Self {
            expected_size: ExpectedSize::new(size),
            canonical: None,
            sha1: Some(sha1),
            sha256: None,
            git_sha1: None,
        }
    }

    pub fn with_sha256(size: u64, sha256: hash::Sha256) -> Self {
        use expected_size::ExpectedSize;

        Self {
            expected_size: ExpectedSize::new(size),
            canonical: None,
            sha1: None,
            sha256: Some(sha256),
            git_sha1: None,
        }
    }

    pub fn with_git_sha1(size: u64, git_sha1: hash::RichGitSha1) -> Self {
        use expected_size::ExpectedSize;

        Self {
            expected_size: ExpectedSize::new(size),
            canonical: None,
            sha1: None,
            sha256: None,
            git_sha1: Some(git_sha1),
        }
    }

    pub fn with_fetch_key(size: u64, key: FetchKey) -> Self {
        use FetchKey::*;
        match key {
            Canonical(id) => Self::with_canonical(size, id),
            Aliased(Alias::Sha1(id)) => Self::with_sha1(size, id),
            Aliased(Alias::GitSha1(id)) => {
                Self::with_git_sha1(size, hash::RichGitSha1::from_sha1(id, "blob", size))
            }
            Aliased(Alias::Sha256(id)) => Self::with_sha256(size, id),
        }
    }
}

/// Fetch the metadata for the underlying content. This will return None if the content does
/// not exist. It might recompute metadata on the fly if the content exists but the metadata does
/// not.
pub async fn get_metadata<B: Blobstore>(
    blobstore: &B,
    ctx: &CoreContext,
    key: &FetchKey,
) -> Result<Option<ContentMetadata>, Error> {
    let maybe_id = key
        .load(ctx, blobstore)
        .await
        .map(Some)
        .or_else(|err| match err {
            LoadableError::Error(err) => Err(err),
            LoadableError::Missing(_) => Ok(None),
        })?;

    match maybe_id {
        Some(id) => metadata::get_metadata(blobstore, ctx, id).await,
        None => Ok(None),
    }
}

/// Fetch the metadata for the underlying content. This will return None if the content does
/// not exist, Some(None) if the metadata does not exist, and Some(Some(ContentMetadata))
/// when metadata found. It will not recompute metadata on the fly
pub async fn get_metadata_readonly<B: Blobstore>(
    blobstore: &B,
    ctx: &CoreContext,
    key: &FetchKey,
) -> Result<Option<Option<ContentMetadata>>, Error> {
    let maybe_id = key
        .load(ctx, blobstore)
        .await
        .map(Some)
        .or_else(|err| match err {
            LoadableError::Error(err) => Err(err),
            LoadableError::Missing(_) => Ok(None),
        })?;

    match maybe_id {
        Some(id) => metadata::get_metadata_readonly(blobstore, ctx, id)
            .await
            .map(Some),
        None => Ok(Some(None)),
    }
}

/// Return true if the given key exists. A successful return means the key definitely
/// either exists or doesn't; an error means the existence could not be determined.
pub async fn exists<B: Blobstore>(
    blobstore: &B,
    ctx: &CoreContext,
    key: &FetchKey,
) -> Result<bool, Error> {
    let maybe_id = key
        .load(ctx, blobstore)
        .await
        .map(Some)
        .or_else(|err| match err {
            LoadableError::Error(err) => Err(err),
            LoadableError::Missing(_) => Ok(None),
        })?;

    match maybe_id {
        Some(id) => blobstore
            .is_present(ctx, &id.blobstore_key())
            .await?
            .fail_if_unsure(),
        None => Ok(false),
    }
}

/// Fetch a file as a stream. This returns either success with a stream of data and file size if
/// the file exists, success with None if it does not exist, or an Error if either existence can't
/// be determined or if opening the file failed. File contents are returned in chunks configured by
/// FilestoreConfig::read_chunk_size - this defines the max chunk size, but they may be shorter
/// (not just the final chunks - any of them). Chunks are guaranteed to have non-zero size.
pub async fn fetch_with_size<'a, B: Blobstore + Clone + 'a>(
    blobstore: B,
    ctx: impl Borrow<CoreContext> + Clone + Send + Sync + 'a,
    key: &FetchKey,
) -> Result<Option<(impl Stream<Item = Result<Bytes, Error>> + 'a, u64)>, Error> {
    let content_id =
        key.load(ctx.borrow(), &blobstore)
            .await
            .map(Some)
            .or_else(|err| match err {
                LoadableError::Error(err) => Err(err),
                LoadableError::Missing(_) => Ok(None),
            })?;

    match content_id {
        Some(content_id) => {
            fetch::fetch_with_size(blobstore, ctx, content_id, fetch::Range::all()).await
        }
        None => Ok(None),
    }
}

/// This function has the same functionality as fetch_with_size, but only
/// returns data for a range within the file.
///
/// Requests for data beyond the end of the file will return only the part of
/// the file that overlaps with the requested range, if any.
pub async fn fetch_range_with_size<'a, B: Blobstore + Clone + 'a>(
    blobstore: B,
    ctx: impl Borrow<CoreContext> + Clone + Send + Sync + 'a,
    key: &FetchKey,
    range: Range,
) -> Result<Option<(impl Stream<Item = Result<Bytes, Error>> + 'a, u64)>, Error> {
    let content_id =
        key.load(ctx.borrow(), &blobstore)
            .await
            .map(Some)
            .or_else(|err| match err {
                LoadableError::Error(err) => Err(err),
                LoadableError::Missing(_) => Ok(None),
            })?;

    match content_id {
        Some(content_id) => fetch::fetch_with_size(blobstore, ctx, content_id, range).await,
        None => Ok(None),
    }
}

/// This function has the same functionality as fetch_with_size, but doesn't return the file size.
pub async fn fetch<'a, B: Blobstore + Clone + 'a>(
    blobstore: B,
    ctx: impl Borrow<CoreContext> + Clone + Send + Sync + 'a,
    key: &FetchKey,
) -> Result<Option<impl Stream<Item = Result<Bytes, Error>> + 'a>, Error> {
    let res = fetch_with_size(blobstore, ctx, key).await?;
    Ok(res.map(|(stream, _len)| stream))
}

/// Fetch the contents of a blob concatenated together. This bad for buffering, and you shouldn't
/// add new callsites. This is only for compatibility with existin callsites.
pub async fn fetch_concat_opt<B: Blobstore>(
    blobstore: &B,
    ctx: &CoreContext,
    key: &FetchKey,
) -> Result<Option<Bytes>, Error> {
    let res = fetch_with_size(blobstore, ctx, key).await?;

    match res {
        Some((stream, len)) => {
            let len = len
                .try_into()
                .map_err(|_| anyhow::format_err!("Cannot fetch file with length {}", len))?;

            let buf = BytesMut::with_capacity(len);

            let bytes = stream
                .try_fold(buf, |mut buffer, chunk| async move {
                    buffer.extend_from_slice(&chunk);
                    Result::<_, Error>::Ok(buffer)
                })
                .await?;

            Ok(Some(bytes.freeze()))
        }
        None => Ok(None),
    }
}

/// Similar to `fetch_concat_opt`, but requires the blob to be present, or errors out.
pub async fn fetch_concat<B: Blobstore>(
    blobstore: &B,
    ctx: &CoreContext,
    key: impl Into<FetchKey>,
) -> Result<Bytes, Error> {
    let key: FetchKey = key.into();
    let bytes = fetch_concat_opt(blobstore, ctx, &key).await?;
    bytes.ok_or_else(|| errors::ErrorKind::MissingContent(key).into())
}

/// Fetch content associated with the key as a stream
///
/// Moslty behaves as the `fetch`, except it is pushing missing content error into stream if
/// data associated with the key was not found.
pub fn fetch_stream<'a, B: Blobstore + Clone + 'a>(
    blobstore: B,
    ctx: impl Borrow<CoreContext> + Clone + Send + Sync + 'a,
    key: impl Into<FetchKey>,
) -> impl Stream<Item = Result<Bytes, Error>> + 'a {
    let key: FetchKey = key.into();

    async move {
        let stream = fetch(blobstore, ctx, &key)
            .await?
            .ok_or(errors::ErrorKind::MissingContent(key))?;
        Result::<_, Error>::Ok(stream)
    }
    .try_flatten_stream()
}

/// This function has the same functionality as fetch_range_with_size, but doesn't return the file size.
pub async fn fetch_range<'a, B: Blobstore>(
    blobstore: &'a B,
    ctx: &'a CoreContext,
    key: &FetchKey,
    range: Range,
) -> Result<Option<impl Stream<Item = Result<Bytes, Error>> + 'a>, Error> {
    let res = fetch_range_with_size(blobstore, ctx, key, range).await?;
    Ok(res.map(|(stream, _len)| stream))
}

/// Fetch the start of a file. Returns a Future that resolves with Some(Bytes) if the file was
/// found, and None otherwise. If Bytes are found, this function is guaranteed to return as many
/// Bytes as requested unless the file is shorter than that.
pub async fn peek<B: Blobstore>(
    blobstore: &B,
    ctx: &CoreContext,
    key: &FetchKey,
    size: usize,
) -> Result<Option<Bytes>, Error> {
    let maybe_stream = fetch(blobstore, ctx, key).await?;

    match maybe_stream {
        None => Ok(None),
        Some(stream) => {
            let mut stream = chunk::ChunkStream::new(stream, size);
            stream.try_next().await
        }
    }
}

/// Store a file from a stream. This is guaranteed atomic - either the store will succeed
/// for the entire file, or it will fail and the file will logically not exist (however
/// there's no guarantee that any partially written parts will be cleaned up).
pub async fn store<B: Blobstore + Clone + 'static>(
    blobstore: &B,
    config: FilestoreConfig,
    ctx: &CoreContext,
    req: &StoreRequest,
    data: impl Stream<Item = Result<Bytes, Error>> + Send,
) -> Result<ContentMetadata, Error> {
    use chunk::Chunks;

    let prepared = match chunk::make_chunks(data, req.expected_size, config.chunk_size) {
        Chunks::Inline(fut) => prepare::prepare_bytes(fut.await?),
        Chunks::Chunked(expected_size, chunks) => {
            prepare::prepare_chunked(
                ctx.clone(),
                blobstore.clone(),
                expected_size,
                chunks,
                config.concurrency,
            )
            .await?
        }
    };

    finalize::finalize(blobstore, ctx, Some(req), prepared).await
}

/// Store a set of bytes, and immediately return their Contentid and size. This function is
/// inefficient for large files, since it will hash the file twice if it's larger than the chunk
/// size. This function is intended as a transition function while we convert writers to streams
/// and refactor them to not expect to be able to obtain the ContentId for the content they
/// uploaded immediately. Do NOT add new callsites.
pub fn store_bytes<B: Blobstore + Clone + 'static>(
    blobstore: &B,
    config: FilestoreConfig,
    ctx: &CoreContext,
    bytes: Bytes,
) -> (
    (ContentId, u64),
    impl Future<Output = Result<(), Error>> + Send,
) {
    // NOTE: Like in other places in the Filestore, we assume that the size of buffers being passed
    // in can be represented in 64 bits (which is OK for the world we live in).

    let content_id = FileContents::content_id_for_bytes(&bytes);
    let size: u64 = bytes.len().try_into().unwrap();

    cloned!(ctx, blobstore);
    let upload = async move {
        store(
            &blobstore,
            config,
            &ctx,
            &StoreRequest::with_canonical(size, content_id),
            stream::once(async move { Ok(bytes) }),
        )
        .await?;

        Ok(())
    };

    ((content_id, size), upload)
}
